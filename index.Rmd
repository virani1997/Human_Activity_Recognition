---
title: "Multinomial Logisitic Regression"
author: "Salman Virani"
date: '2022-05-18'
output: 
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har

We want to predict the manner in which people do the exercise. This is the “classes” variable in the training set. We may use any of the other variables to predict with. We create a report describing all the analysis and in particular the prediction of 20 different test cases.

The five different ‘classe’ factors in this dataset are: * Exactly according to the specification (Class A) * Throwing the elbows to the front (Class B) * Lifting the dumbbell only halfway (Class C) * Lowering the dumbbell only halfway (Class D) * Throwing the hips to the front (Class E)

## Libraries

We will be using the following libraries from CRAN; readr, dplyr, nnet, and caret. **readr** allows us to read the data into R workspace, **dplyr** allows data manipulation, **'caret** allows us to get the confusion matrix for our model, **nnet** allows access to the neural network and log model algorithms, and **rsample** lets us split the data into training and validating.

```{r echo=FALSE, comment=""}
library(readr)
library(dplyr)
library(nnet)
library(caret)
library(rsample)
```

## Data

If the data is not available, then it will be downloaded and read into R. At this point, train and test data is loaded in the environment. Note that our test data doesn't have predefined labels. We will divide our train data into training and validation to get the in sample and out sample measures of our model. Test data will be used to apply our model like in real time. A basic snap of our train data is as below.It has 160 variables, among which our target varaible is **classe**

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=""}
file_path1 <- "./data/training.csv"
file_path2 <- "./data/testing.csv"

if(!file.exists(file_path1)){
  dir.create("./data")
  url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(url, file_path1)
}

if(!file.exists(file_path2)){
  dir.create("./data")
  url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  download.file(url2, destfile = file_path2)
}

train <- read_csv("./data/training.csv")
testing <- read_csv("./data/testing.csv")

head(train)
```

# Data Wrangling

Some unnecessary columns needs to be removed before we can move on to modeling process. Firstly, the columns that have more than 19000 NAs are removed. Secondly, **classe** column has been coverted to factors. Thirdly, the initial train data is split into training and validating. Lastly, some unnecessary columns like id etc have been removed.

```{r echo=FALSE, comment=""}
remove_col <- colSums(is.na(train)) > 19000
train <- train[,!remove_col]
testing <- testing[,!remove_col]

train$classe <- as.factor(train$classe)

initial_split <- initial_split(train, prop = 0.8)
training <- training(initial_split)
validating <- testing(initial_split)

training <- training %>%
  select(-1, -3:-7)

validating <- validating %>%
  select(-1, -3:-7)

testing <- testing %>%
  select(-1, -3:-7)
```

# Multinomial Logistic Model

```{r echo=FALSE, comment=""}
model <- multinom(classe~., data = training)

training <- training %>%
  mutate(pred_multilog = predict(model, newdata = training))

validating <- validating %>%
  mutate(pred_multilog = predict(model, newdata = validating))

```

## Accuracy Measures

```{r echo=FALSE, comment=""}
conf_mat_training <- table(actual = training$classe, predicted = training$pred_multilog)

conf_mat_validating <- table(actual = validating$classe, predicted = validating$pred_multilog)

plot(conf_mat_training)
confusionMatrix(conf_mat_training)

plot(conf_mat_validating)
confusionMatrix(conf_mat_validating)

```

## Test Data

We dont have the labels for the test data. We predict the positions of the test data and export the results into a csv file.

```{r echo=FALSE, comment=""}
testing <- testing %>%
  mutate(pred_multilog = predict(model, newdata = testing))

testing %>%
  select(pred_multilog)

testing_pred_multilog <- as.data.frame(testing$pred_multilog)

write_csv(testing_pred_multilog, "testing_results.csv")
```

# Conclusion

I wished to apply some other models like decision trees, and random forests from which I expected to get better accuracy. However, computation power limitation doesn't allow us to do so at this time. 
