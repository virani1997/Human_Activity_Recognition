---
title: "Classifation Algorithms to detect Human Activity"
author: "Salman Virani"
date: '2022-05-18'
output: 
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har

We want to predict the manner in which people do the exercise. This is the “classes” variable in the training set. We may use any of the other variables to predict with. We create a report describing all the analysis and in particular the prediction of 20 different test cases.

The five different ‘classe’ factors in this dataset are: * Exactly according to the specification (Class A) * Throwing the elbows to the front (Class B) * Lifting the dumbbell only halfway (Class C) * Lowering the dumbbell only halfway (Class D) * Throwing the hips to the front (Class E)

Our train data comes from this [source](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and the test data comes from this [source](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv). Test data doesn't have labels so we can not check the accuracy of the model with. We need to split our train data into training and validating so we can check out the in sample and out sample accuracy measures. Our test data will be used to predict the labels using our models like in real time, and compare the results of different models. 

## Libraries

We will be using the following libraries from CRAN; readr, dplyr, nnet, and caret. **readr** allows us to read the data into R workspace, **dplyr** allows data manipulation, **'caret** allows us to get the confusion matrix for our model, **nnet** allows access to the neural network and log model algorithms, and **rsample** lets us split the data into training and validating.

```{r echo=FALSE, comment=""}
library(readr)
library(dplyr)
library(nnet)
library(caret)
library(rsample)
```

## Data

If the data is not available, then it will be downloaded and read into R. At this point, train and test data is loaded in the environment. Note that our test data doesn't have predefined labels. We will divide our train data into training and validation to get the in sample and out sample measures of our model. Test data will be used to apply our model like in real time. A basic snap of our train data is as below.It has 160 variables, among which our target varaible is **classe**

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=""}
file_path1 <- "./data/training.csv"
file_path2 <- "./data/testing.csv"

if(!file.exists(file_path1)){
  dir.create("./data")
  url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(url, file_path1)
}

if(!file.exists(file_path2)){
  dir.create("./data")
  url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  download.file(url2, destfile = file_path2)
}

train <- read_csv("./data/training.csv")
testing <- read_csv("./data/testing.csv")

head(train)
```

# Data Wrangling

Some unnecessary columns needs to be removed before we can move on to modeling process. Firstly, the columns that have more than 19000 NAs are removed. Secondly, **classe** column has been coverted to factors. Thirdly, the initial train data is split into training and validating. Lastly, some unnecessary columns like id etc have been removed.

```{r echo=FALSE, comment=""}
remove_col <- colSums(is.na(train)) > 19000
train <- train[,!remove_col]
testing <- testing[,!remove_col]

train$classe <- as.factor(train$classe)

initial_split <- initial_split(train, prop = 0.8)
training <- training(initial_split)
validating <- testing(initial_split)

training <- training %>%
  select(-1:-7)

validating <- validating %>%
  select(-1:-7)

testing <- testing %>%
  select(-1:-7)
```

# Multinomial Logistic Model

```{r echo=FALSE, comment="", cache=T}
model <- multinom(classe~., data = training)

training <- training %>%
  mutate(pred_multilog = predict(model, newdata = training))

validating <- validating %>%
  mutate(pred_multilog = predict(model, newdata = validating))

```

## Accuracy Measures

```{r echo=FALSE, comment=""}
conf_mat_multilog_training <- table(actual = training$classe, predicted = training$pred_multilog)

conf_mat_multilog_validating <- table(actual = validating$classe, predicted = validating$pred_multilog)

plot(conf_mat_multilog_training)
confusionMatrix(conf_mat_multilog_training)

plot(conf_mat_multilog_validating)
confusionMatrix(conf_mat_multilog_validating)
```

## Predictive Analytics 

We dont have the labels for the test data. We predict the positions of the test data and export the results into a csv file.

```{r echo=FALSE, comment=""}
testing <- testing %>%
  mutate(pred_multilog = predict(model, newdata = testing))

```

# KNN

```{r, cache=TRUE, echo=FALSE, comment=""}
model2 <- train(classe~., data = training, method = "knn")
model2
```
## Accuracy measures

```{r, echo=FALSE, comment=""}
training$pred_knn <- predict(model2, newdata = training)

conf_mat_knn_training <- table(actual = training$classe,
                               training$pred_knn)

plot(conf_mat_knn_training)
confusionMatrix(conf_mat_knn_training)

validating$pred_knn <- predict(model2, newdata = validating)

conf_mat_knn_validating <- table(actual = validating$classe, 
                                 predicted = validating$pred_knn)

plot(conf_mat_knn_validating)
confusionMatrix(conf_mat_knn_validating)
```

## Predictive Analysis and Comparison of KNN and Multinomial Logistic Regression

Some results of knn and multinomial logistic regression are compared. We can see that some results are same, while other are different. We will prefer the results of KNN as it provides better insample and out sample accurayc measures. 

```{r echo=FALSE, comment=""}
testing <- testing %>%
  mutate(pred_knn = predict(model2, newdata = testing))

testing_pred <- tibble(multinomail = testing$pred_multilog,
                       knn = testing$pred_knn)
head(testing_pred)

write_csv(testing_pred, "testing_results.csv")
```

# Conclusion

We see K Nearest Neighbors provide a much better accuracy model compared to Multinational Logistic Regression. Some other algorithms, like decision trees, random forests, support vector machine might provide interesting results. However, computation power limitation doesn't allow us to include those algorithms at this moment.

I hope you have enjoyed and learned from this project. You can support me my keeping yourself up to date with future projects. For that, kindly follow my [GitHub](https://github.com/virani1997) and [Twitter](https://twitter.com/SalmanVirani6) profiles. 

